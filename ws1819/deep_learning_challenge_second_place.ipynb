{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "# import needed packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, GroupShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, LSTM, TimeDistributed\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "from keras.regularizers import l2, l1, L1L2\n",
    "from keras.initializers import Constant, RandomNormal, RandomUniform, TruncatedNormal\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from keras.losses import mean_squared_error\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = np.load(\"homework_09_data.npz\")\n",
    "\n",
    "train_data = loader['train_data']\n",
    "train_labels = loader['train_labels']\n",
    "\n",
    "val_data = loader['val_data']\n",
    "val_labels = loader['val_labels']\n",
    "\n",
    "learn_data = np.concatenate((train_data, val_data), axis=0)\n",
    "learn_labels = np.concatenate((train_labels, val_labels), axis=0)\n",
    "\n",
    "test_data = loader['test_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julius/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/julius/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/julius/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/julius/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/julius/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/julius/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/julius/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/julius/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "val_data = scaler.fit_transform(val_data)\n",
    "learn_data = scaler.fit_transform(learn_data)\n",
    "test_data = scaler.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch annoying sklearn warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "\n",
    "USE_DROPOUT = False\n",
    "USE_BATCH_NORM = True\n",
    "CLIP_GRADIENTS = False\n",
    "\n",
    "clip_value = 0.05\n",
    "lr = 0.01\n",
    "dropout_rate = 0.1\n",
    "loss_func = 'categorical_crossentropy'\n",
    "epochs = 250\n",
    "weight_initializer =TruncatedNormal(mean=0.0, stddev=0.05)\n",
    "bias_initializer = Constant(0.1)\n",
    "\n",
    "if CLIP_GRADIENTS:\n",
    "    optimizer = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, clipvalue=clip_value)\n",
    "else:\n",
    "    optimizer = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None)\n",
    "    \n",
    "input_dim = np.shape(train_data[0])[0]\n",
    "weight_decay =  l2(0.)\n",
    "metrics = ['accuracy']\n",
    "hidden_activation = 'relu'\n",
    "output_activation = 'softmax'\n",
    "batch_size = 128\n",
    "hidden_layer_size = 64\n",
    "num_classes = np.shape(train_labels[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build feedforward net\n",
    "model = Sequential()\n",
    "\n",
    "# input layer\n",
    "model.add(Dense(512,\n",
    "                activation=hidden_activation,\n",
    "                kernel_initializer=weight_initializer,\n",
    "                kernel_regularizer=weight_decay,\n",
    "                bias_initializer=bias_initializer,\n",
    "                input_dim=input_dim))\n",
    "\n",
    "# dropout layer\n",
    "if USE_DROPOUT:\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "# batch norm layer\n",
    "if USE_BATCH_NORM:\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "# hidden layer 1\n",
    "model.add(Dense(256,\n",
    "                activation=hidden_activation,\n",
    "                kernel_initializer=weight_initializer,\n",
    "                kernel_regularizer=weight_decay,\n",
    "                bias_initializer=bias_initializer,\n",
    "                input_dim=input_dim))\n",
    "\n",
    "# dropout layer\n",
    "if USE_DROPOUT:\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "# batch norm layer\n",
    "if USE_BATCH_NORM:\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "# hidden layer 2\n",
    "model.add(Dense(256,\n",
    "                activation=hidden_activation,\n",
    "                kernel_initializer=weight_initializer,\n",
    "                kernel_regularizer=weight_decay,\n",
    "                bias_initializer=bias_initializer,\n",
    "                input_dim=input_dim))\n",
    "\n",
    "# dropout layer\n",
    "if USE_DROPOUT:\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "# batch norm layer\n",
    "if USE_BATCH_NORM:\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "# hidden layer 2\n",
    "model.add(Dense(128,\n",
    "                activation=hidden_activation,\n",
    "                kernel_initializer=weight_initializer,\n",
    "                kernel_regularizer=weight_decay,\n",
    "                bias_initializer=bias_initializer,\n",
    "                input_dim=input_dim))\n",
    "\n",
    "# dropout layer\n",
    "if USE_DROPOUT:\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "# batch norm layer\n",
    "if USE_BATCH_NORM:\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "# hidden layer 3\n",
    "model.add(Dense(64,\n",
    "                activation=hidden_activation,\n",
    "                kernel_initializer=weight_initializer,\n",
    "                kernel_regularizer=weight_decay,\n",
    "                bias_initializer=bias_initializer,\n",
    "                input_dim=input_dim))\n",
    "\n",
    "# dropout layer\n",
    "if USE_DROPOUT:\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "# batch norm layer\n",
    "if USE_BATCH_NORM:\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "# hidden layer 4\n",
    "model.add(Dense(32,\n",
    "                activation=hidden_activation,\n",
    "                kernel_initializer=weight_initializer,\n",
    "                kernel_regularizer=weight_decay,\n",
    "                bias_initializer=bias_initializer,\n",
    "                input_dim=input_dim))\n",
    "\n",
    "# dropout layer\n",
    "if USE_DROPOUT:\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "# batch norm layer\n",
    "if USE_BATCH_NORM:\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(num_classes,\n",
    "                activation=output_activation,\n",
    "                kernel_initializer=weight_initializer,\n",
    "                kernel_regularizer=weight_decay,\n",
    "                bias_initializer=bias_initializer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 512)               61952     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 307,494\n",
      "Trainable params: 304,998\n",
      "Non-trainable params: 2,496\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def lr_step_decay(epoch):\n",
    "    global lr\n",
    "    initial_lrate = lr\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * np.power(drop, np.floor((1+epoch)/epochs_drop))\n",
    "    \n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callback functions during model fitting in each epoch\n",
    "# use early stopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=50, restore_best_weights=True)\n",
    "\n",
    "lrate = LearningRateScheduler(lr_step_decay)\n",
    "\n",
    "callbacks = [early_stopping, lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4819 samples, validate on 1606 samples\n",
      "Epoch 1/250\n",
      "4819/4819 [==============================] - 3s 614us/step - loss: 0.6932 - acc: 0.7435 - val_loss: 1.5024 - val_acc: 0.7055\n",
      "Epoch 2/250\n",
      "4819/4819 [==============================] - 1s 107us/step - loss: 0.3865 - acc: 0.8616 - val_loss: 0.7495 - val_acc: 0.8462\n",
      "Epoch 3/250\n",
      "4819/4819 [==============================] - 1s 115us/step - loss: 0.3012 - acc: 0.8989 - val_loss: 0.5970 - val_acc: 0.8624\n",
      "Epoch 4/250\n",
      "4819/4819 [==============================] - 1s 113us/step - loss: 0.2608 - acc: 0.9137 - val_loss: 0.4591 - val_acc: 0.8773\n",
      "Epoch 5/250\n",
      "4819/4819 [==============================] - 1s 111us/step - loss: 0.2023 - acc: 0.9328 - val_loss: 0.3211 - val_acc: 0.9228\n",
      "Epoch 6/250\n",
      "4819/4819 [==============================] - 1s 111us/step - loss: 0.1919 - acc: 0.9369 - val_loss: 0.4088 - val_acc: 0.8998\n",
      "Epoch 7/250\n",
      "4819/4819 [==============================] - 1s 114us/step - loss: 0.1614 - acc: 0.9535 - val_loss: 0.2729 - val_acc: 0.9184\n",
      "Epoch 8/250\n",
      "4819/4819 [==============================] - 1s 113us/step - loss: 0.1726 - acc: 0.9465 - val_loss: 0.3466 - val_acc: 0.9122\n",
      "Epoch 9/250\n",
      "4819/4819 [==============================] - 1s 113us/step - loss: 0.1483 - acc: 0.9529 - val_loss: 0.4017 - val_acc: 0.8861\n",
      "Epoch 10/250\n",
      "4819/4819 [==============================] - 1s 113us/step - loss: 0.1310 - acc: 0.9570 - val_loss: 0.1913 - val_acc: 0.9539\n",
      "Epoch 11/250\n",
      "4819/4819 [==============================] - 1s 115us/step - loss: 0.1011 - acc: 0.9699 - val_loss: 0.2115 - val_acc: 0.9489\n",
      "Epoch 12/250\n",
      "4819/4819 [==============================] - 1s 115us/step - loss: 0.0911 - acc: 0.9714 - val_loss: 0.1938 - val_acc: 0.9440\n",
      "Epoch 13/250\n",
      "4819/4819 [==============================] - 1s 118us/step - loss: 0.0936 - acc: 0.9697 - val_loss: 0.1896 - val_acc: 0.9552\n",
      "Epoch 14/250\n",
      "4819/4819 [==============================] - 1s 113us/step - loss: 0.0799 - acc: 0.9768 - val_loss: 0.1854 - val_acc: 0.9539\n",
      "Epoch 15/250\n",
      "4819/4819 [==============================] - 1s 117us/step - loss: 0.0861 - acc: 0.9747 - val_loss: 0.2276 - val_acc: 0.9247\n",
      "Epoch 16/250\n",
      "4819/4819 [==============================] - 1s 118us/step - loss: 0.0705 - acc: 0.9753 - val_loss: 0.2296 - val_acc: 0.9240\n",
      "Epoch 17/250\n",
      "4819/4819 [==============================] - 1s 116us/step - loss: 0.0764 - acc: 0.9759 - val_loss: 0.2136 - val_acc: 0.9440\n",
      "Epoch 18/250\n",
      "4819/4819 [==============================] - 1s 114us/step - loss: 0.0716 - acc: 0.9780 - val_loss: 0.2625 - val_acc: 0.9197\n",
      "Epoch 19/250\n",
      "4819/4819 [==============================] - 1s 114us/step - loss: 0.0758 - acc: 0.9768 - val_loss: 0.1985 - val_acc: 0.9533\n",
      "Epoch 20/250\n",
      "4819/4819 [==============================] - 1s 115us/step - loss: 0.0575 - acc: 0.9817 - val_loss: 0.2086 - val_acc: 0.9440\n",
      "Epoch 21/250\n",
      "4819/4819 [==============================] - 1s 114us/step - loss: 0.0457 - acc: 0.9840 - val_loss: 0.1779 - val_acc: 0.9564\n",
      "Epoch 22/250\n",
      "4819/4819 [==============================] - 1s 110us/step - loss: 0.0424 - acc: 0.9869 - val_loss: 0.1792 - val_acc: 0.9589\n",
      "Epoch 23/250\n",
      "4819/4819 [==============================] - 1s 111us/step - loss: 0.0361 - acc: 0.9896 - val_loss: 0.1840 - val_acc: 0.9577\n",
      "Epoch 24/250\n",
      "4819/4819 [==============================] - 1s 112us/step - loss: 0.0409 - acc: 0.9861 - val_loss: 0.1874 - val_acc: 0.9521\n",
      "Epoch 25/250\n",
      "4819/4819 [==============================] - 1s 115us/step - loss: 0.0515 - acc: 0.9830 - val_loss: 0.1853 - val_acc: 0.9533\n",
      "Epoch 26/250\n",
      "4819/4819 [==============================] - 1s 116us/step - loss: 0.0477 - acc: 0.9855 - val_loss: 0.1972 - val_acc: 0.9608\n",
      "Epoch 27/250\n",
      "4819/4819 [==============================] - 1s 115us/step - loss: 0.0327 - acc: 0.9896 - val_loss: 0.2232 - val_acc: 0.9521\n",
      "Epoch 28/250\n",
      "4819/4819 [==============================] - 1s 117us/step - loss: 0.0324 - acc: 0.9905 - val_loss: 0.1891 - val_acc: 0.9465\n",
      "Epoch 29/250\n",
      "4819/4819 [==============================] - 1s 117us/step - loss: 0.0273 - acc: 0.9938 - val_loss: 0.1875 - val_acc: 0.9601\n",
      "Epoch 30/250\n",
      "4819/4819 [==============================] - 1s 115us/step - loss: 0.0216 - acc: 0.9946 - val_loss: 0.2060 - val_acc: 0.9415\n",
      "Epoch 31/250\n",
      "4819/4819 [==============================] - 1s 114us/step - loss: 0.0243 - acc: 0.9921 - val_loss: 0.1808 - val_acc: 0.9583\n",
      "Epoch 32/250\n",
      "4819/4819 [==============================] - 1s 114us/step - loss: 0.0232 - acc: 0.9932 - val_loss: 0.1968 - val_acc: 0.9608\n",
      "Epoch 33/250\n",
      "4819/4819 [==============================] - 1s 117us/step - loss: 0.0194 - acc: 0.9936 - val_loss: 0.2109 - val_acc: 0.9539\n",
      "Epoch 34/250\n",
      "4819/4819 [==============================] - 1s 115us/step - loss: 0.0215 - acc: 0.9925 - val_loss: 0.1954 - val_acc: 0.9595\n",
      "Epoch 35/250\n",
      "4819/4819 [==============================] - 1s 115us/step - loss: 0.0211 - acc: 0.9925 - val_loss: 0.2039 - val_acc: 0.9589\n",
      "Epoch 36/250\n",
      "4819/4819 [==============================] - 1s 122us/step - loss: 0.0168 - acc: 0.9961 - val_loss: 0.1983 - val_acc: 0.9564\n",
      "Epoch 37/250\n",
      "4819/4819 [==============================] - 1s 122us/step - loss: 0.0209 - acc: 0.9948 - val_loss: 0.2069 - val_acc: 0.9601\n",
      "Epoch 38/250\n",
      "4819/4819 [==============================] - 1s 122us/step - loss: 0.0135 - acc: 0.9969 - val_loss: 0.2075 - val_acc: 0.9570\n",
      "Epoch 39/250\n",
      "4819/4819 [==============================] - 1s 125us/step - loss: 0.0194 - acc: 0.9948 - val_loss: 0.2224 - val_acc: 0.9402\n",
      "Epoch 40/250\n",
      "4819/4819 [==============================] - 1s 120us/step - loss: 0.0153 - acc: 0.9958 - val_loss: 0.1897 - val_acc: 0.9614\n",
      "Epoch 41/250\n",
      "4819/4819 [==============================] - 1s 120us/step - loss: 0.0130 - acc: 0.9969 - val_loss: 0.2026 - val_acc: 0.9589\n",
      "Epoch 42/250\n",
      "4819/4819 [==============================] - 1s 119us/step - loss: 0.0149 - acc: 0.9956 - val_loss: 0.2031 - val_acc: 0.9601\n",
      "Epoch 43/250\n",
      "4819/4819 [==============================] - 1s 119us/step - loss: 0.0248 - acc: 0.9925 - val_loss: 0.2014 - val_acc: 0.9633\n",
      "Epoch 44/250\n",
      "4819/4819 [==============================] - 1s 118us/step - loss: 0.0121 - acc: 0.9963 - val_loss: 0.2042 - val_acc: 0.9608\n",
      "Epoch 45/250\n",
      "4819/4819 [==============================] - 1s 120us/step - loss: 0.0130 - acc: 0.9965 - val_loss: 0.2031 - val_acc: 0.9589\n",
      "Epoch 46/250\n",
      "4819/4819 [==============================] - 1s 120us/step - loss: 0.0129 - acc: 0.9965 - val_loss: 0.2009 - val_acc: 0.9608\n",
      "Epoch 47/250\n",
      "4819/4819 [==============================] - 1s 121us/step - loss: 0.0141 - acc: 0.9965 - val_loss: 0.1992 - val_acc: 0.9633\n",
      "Epoch 48/250\n",
      "4819/4819 [==============================] - 1s 120us/step - loss: 0.0137 - acc: 0.9961 - val_loss: 0.2087 - val_acc: 0.9589\n",
      "Epoch 49/250\n",
      "4819/4819 [==============================] - 1s 119us/step - loss: 0.0133 - acc: 0.9973 - val_loss: 0.2061 - val_acc: 0.9614\n",
      "Epoch 50/250\n",
      "4819/4819 [==============================] - 1s 123us/step - loss: 0.0115 - acc: 0.9969 - val_loss: 0.2040 - val_acc: 0.9620\n",
      "Epoch 51/250\n",
      "4819/4819 [==============================] - 1s 121us/step - loss: 0.0101 - acc: 0.9971 - val_loss: 0.2044 - val_acc: 0.9595\n",
      "Epoch 52/250\n",
      "4819/4819 [==============================] - 1s 119us/step - loss: 0.0111 - acc: 0.9969 - val_loss: 0.1976 - val_acc: 0.9633\n",
      "Epoch 53/250\n",
      "4819/4819 [==============================] - 1s 119us/step - loss: 0.0117 - acc: 0.9971 - val_loss: 0.1986 - val_acc: 0.9626\n",
      "Epoch 54/250\n",
      "4819/4819 [==============================] - 1s 121us/step - loss: 0.0105 - acc: 0.9973 - val_loss: 0.2005 - val_acc: 0.9626\n",
      "Epoch 55/250\n",
      "4819/4819 [==============================] - 1s 120us/step - loss: 0.0126 - acc: 0.9967 - val_loss: 0.2000 - val_acc: 0.9614\n",
      "Epoch 56/250\n",
      "4819/4819 [==============================] - 1s 123us/step - loss: 0.0103 - acc: 0.9973 - val_loss: 0.1986 - val_acc: 0.9620\n",
      "Epoch 57/250\n",
      "4819/4819 [==============================] - 1s 122us/step - loss: 0.0147 - acc: 0.9956 - val_loss: 0.2063 - val_acc: 0.9595\n",
      "Epoch 58/250\n",
      "4819/4819 [==============================] - 1s 121us/step - loss: 0.0102 - acc: 0.9969 - val_loss: 0.2035 - val_acc: 0.9601\n",
      "Epoch 59/250\n",
      "4819/4819 [==============================] - 1s 120us/step - loss: 0.0097 - acc: 0.9977 - val_loss: 0.2051 - val_acc: 0.9583\n",
      "Epoch 60/250\n",
      "4819/4819 [==============================] - 1s 104us/step - loss: 0.0104 - acc: 0.9975 - val_loss: 0.2054 - val_acc: 0.9589\n",
      "Epoch 61/250\n",
      "4819/4819 [==============================] - 0s 101us/step - loss: 0.0106 - acc: 0.9971 - val_loss: 0.2068 - val_acc: 0.9577\n",
      "Epoch 62/250\n",
      "4819/4819 [==============================] - 0s 102us/step - loss: 0.0086 - acc: 0.9977 - val_loss: 0.2073 - val_acc: 0.9570\n",
      "Epoch 63/250\n",
      "4819/4819 [==============================] - 0s 102us/step - loss: 0.0078 - acc: 0.9979 - val_loss: 0.2072 - val_acc: 0.9583\n",
      "Epoch 64/250\n",
      "4819/4819 [==============================] - 0s 102us/step - loss: 0.0090 - acc: 0.9973 - val_loss: 0.2073 - val_acc: 0.9583\n",
      "Epoch 65/250\n",
      "4819/4819 [==============================] - 0s 102us/step - loss: 0.0117 - acc: 0.9967 - val_loss: 0.2083 - val_acc: 0.9570\n",
      "Epoch 66/250\n",
      "4819/4819 [==============================] - 0s 102us/step - loss: 0.0097 - acc: 0.9969 - val_loss: 0.2085 - val_acc: 0.9589\n",
      "Epoch 67/250\n",
      "4819/4819 [==============================] - 1s 106us/step - loss: 0.0089 - acc: 0.9975 - val_loss: 0.2103 - val_acc: 0.9577\n",
      "Epoch 68/250\n",
      "4819/4819 [==============================] - 1s 112us/step - loss: 0.0087 - acc: 0.9977 - val_loss: 0.2083 - val_acc: 0.9589\n",
      "Epoch 69/250\n",
      "4819/4819 [==============================] - 1s 106us/step - loss: 0.0072 - acc: 0.9981 - val_loss: 0.2101 - val_acc: 0.9589\n",
      "Epoch 70/250\n",
      "4819/4819 [==============================] - 1s 111us/step - loss: 0.0097 - acc: 0.9975 - val_loss: 0.2098 - val_acc: 0.9595\n",
      "Epoch 71/250\n",
      "4819/4819 [==============================] - 0s 104us/step - loss: 0.0065 - acc: 0.9983 - val_loss: 0.2107 - val_acc: 0.9583\n",
      "Epoch 72/250\n",
      "4819/4819 [==============================] - 1s 105us/step - loss: 0.0122 - acc: 0.9971 - val_loss: 0.2106 - val_acc: 0.9589\n",
      "Epoch 73/250\n",
      "4819/4819 [==============================] - 1s 105us/step - loss: 0.0102 - acc: 0.9975 - val_loss: 0.2105 - val_acc: 0.9595\n",
      "Epoch 74/250\n",
      "4819/4819 [==============================] - 1s 168us/step - loss: 0.0093 - acc: 0.9975 - val_loss: 0.2099 - val_acc: 0.9595\n",
      "Epoch 75/250\n",
      "4819/4819 [==============================] - 1s 124us/step - loss: 0.0080 - acc: 0.9977 - val_loss: 0.2115 - val_acc: 0.9595\n",
      "Epoch 76/250\n",
      "4819/4819 [==============================] - 1s 118us/step - loss: 0.0071 - acc: 0.9981 - val_loss: 0.2108 - val_acc: 0.9608\n",
      "Epoch 77/250\n",
      "4819/4819 [==============================] - 1s 134us/step - loss: 0.0074 - acc: 0.9979 - val_loss: 0.2104 - val_acc: 0.9608\n",
      "Epoch 78/250\n",
      "4819/4819 [==============================] - 1s 126us/step - loss: 0.0086 - acc: 0.9983 - val_loss: 0.2104 - val_acc: 0.9595\n",
      "Epoch 79/250\n",
      "4819/4819 [==============================] - 1s 121us/step - loss: 0.0089 - acc: 0.9973 - val_loss: 0.2115 - val_acc: 0.9595\n",
      "Epoch 80/250\n",
      "4819/4819 [==============================] - 1s 121us/step - loss: 0.0083 - acc: 0.9977 - val_loss: 0.2125 - val_acc: 0.9595\n",
      "Epoch 81/250\n",
      "4819/4819 [==============================] - 1s 122us/step - loss: 0.0081 - acc: 0.9979 - val_loss: 0.2132 - val_acc: 0.9595\n",
      "Epoch 82/250\n",
      "4819/4819 [==============================] - 1s 120us/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.2139 - val_acc: 0.9595\n",
      "Epoch 83/250\n",
      "4819/4819 [==============================] - 1s 127us/step - loss: 0.0087 - acc: 0.9975 - val_loss: 0.2136 - val_acc: 0.9595\n",
      "Epoch 84/250\n",
      "4819/4819 [==============================] - 1s 121us/step - loss: 0.0084 - acc: 0.9977 - val_loss: 0.2136 - val_acc: 0.9589\n",
      "Epoch 85/250\n",
      "4819/4819 [==============================] - 1s 120us/step - loss: 0.0086 - acc: 0.9979 - val_loss: 0.2136 - val_acc: 0.9595\n",
      "Epoch 86/250\n",
      "4819/4819 [==============================] - 1s 127us/step - loss: 0.0093 - acc: 0.9977 - val_loss: 0.2135 - val_acc: 0.9595\n",
      "Epoch 87/250\n",
      "4819/4819 [==============================] - 1s 163us/step - loss: 0.0085 - acc: 0.9979 - val_loss: 0.2137 - val_acc: 0.9601\n",
      "Epoch 88/250\n",
      "4819/4819 [==============================] - 1s 118us/step - loss: 0.0094 - acc: 0.9977 - val_loss: 0.2141 - val_acc: 0.9589\n",
      "Epoch 89/250\n",
      "4819/4819 [==============================] - 1s 147us/step - loss: 0.0101 - acc: 0.9973 - val_loss: 0.2157 - val_acc: 0.9595\n",
      "Epoch 90/250\n",
      "4819/4819 [==============================] - 1s 123us/step - loss: 0.0081 - acc: 0.9977 - val_loss: 0.2156 - val_acc: 0.9589\n",
      "Epoch 91/250\n",
      "4819/4819 [==============================] - 1s 138us/step - loss: 0.0070 - acc: 0.9985 - val_loss: 0.2155 - val_acc: 0.9595\n",
      "Epoch 92/250\n",
      "4819/4819 [==============================] - 1s 127us/step - loss: 0.0086 - acc: 0.9977 - val_loss: 0.2155 - val_acc: 0.9595\n",
      "Epoch 93/250\n",
      "4819/4819 [==============================] - 1s 131us/step - loss: 0.0132 - acc: 0.9969 - val_loss: 0.2150 - val_acc: 0.9601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd4c0f966d8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile & Train the network\n",
    "\n",
    "model.compile(loss=loss_func, \n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics)\n",
    "\n",
    "model.fit(train_data, train_labels,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          callbacks=callbacks,\n",
    "          validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss of model: 0.20136991858089912\n",
      "Accuracy of model: 0.9632627646326276\n"
     ]
    }
   ],
   "source": [
    "# evaluate loss and accuracy of model\n",
    "\n",
    "loss, acc = model.evaluate(x=val_data, y=val_labels, verbose=0)\n",
    "print(\"Prediction loss of model: {}\".format(loss))\n",
    "print(\"Accuracy of model: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1-Score: 0.9598865478537992\n",
      "Micro F1-Score: 0.9632627646326276\n"
     ]
    }
   ],
   "source": [
    "### Neural Net results\n",
    "# evaulate macro and micro f1-score of model\n",
    "\n",
    "prob_predictions = model.predict(val_data, batch_size=None, verbose=0, steps=None)\n",
    "\n",
    "y_val_converted = val_labels\n",
    "hard_predictions = np.argmax(prob_predictions, axis=1)\n",
    "y_val_converted = np.argmax(val_labels, axis=1)\n",
    "\n",
    "macro_f1 = f1_score(y_val_converted, hard_predictions, average='macro') \n",
    "micro_f1 = f1_score(y_val_converted, hard_predictions, average='micro') \n",
    "\n",
    "print(\"Macro F1-Score: {}\".format(macro_f1))\n",
    "print(\"Micro F1-Score: {}\".format(micro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 6 artists>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bar plot of validation set class distribution\n",
    "\n",
    "predicted_samples_per_class = {}\n",
    "\n",
    "for cls in range(num_classes):\n",
    "    predicted_samples_per_class[cls] = np.sum(y_val_converted == cls)\n",
    "\n",
    "plt.bar(predicted_samples_per_class.keys(), predicted_samples_per_class.values(), color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 6 artists>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bar plot of predictions per class\n",
    "\n",
    "predicted_samples_per_class = {}\n",
    "\n",
    "for cls in range(num_classes):\n",
    "    predicted_samples_per_class[cls] = np.sum(hard_predictions == cls)\n",
    "\n",
    "plt.bar(predicted_samples_per_class.keys(), predicted_samples_per_class.values(), color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6425/6425 [==============================] - 3s 418us/step - loss: 0.3396 - acc: 0.8943\n",
      "Epoch 2/100\n",
      "6425/6425 [==============================] - 1s 97us/step - loss: 0.2268 - acc: 0.9296\n",
      "Epoch 3/100\n",
      "6425/6425 [==============================] - 1s 101us/step - loss: 0.1533 - acc: 0.9518\n",
      "Epoch 4/100\n",
      "6425/6425 [==============================] - 1s 102us/step - loss: 0.1437 - acc: 0.9528\n",
      "Epoch 5/100\n",
      "6425/6425 [==============================] - 1s 103us/step - loss: 0.1136 - acc: 0.9639\n",
      "Epoch 6/100\n",
      "6425/6425 [==============================] - 1s 112us/step - loss: 0.0948 - acc: 0.9718\n",
      "Epoch 7/100\n",
      "6425/6425 [==============================] - 1s 106us/step - loss: 0.0973 - acc: 0.9693\n",
      "Epoch 8/100\n",
      "6425/6425 [==============================] - 1s 102us/step - loss: 0.0928 - acc: 0.9721\n",
      "Epoch 9/100\n",
      "6425/6425 [==============================] - 1s 104us/step - loss: 0.1099 - acc: 0.9656\n",
      "Epoch 10/100\n",
      "6425/6425 [==============================] - 1s 112us/step - loss: 0.0988 - acc: 0.9684\n",
      "Epoch 11/100\n",
      "6425/6425 [==============================] - 1s 118us/step - loss: 0.0621 - acc: 0.9812\n",
      "Epoch 12/100\n",
      "6425/6425 [==============================] - 1s 114us/step - loss: 0.0600 - acc: 0.9804\n",
      "Epoch 13/100\n",
      "6425/6425 [==============================] - 1s 126us/step - loss: 0.0621 - acc: 0.9824\n",
      "Epoch 14/100\n",
      "6425/6425 [==============================] - 1s 105us/step - loss: 0.0634 - acc: 0.9798\n",
      "Epoch 15/100\n",
      "6425/6425 [==============================] - 1s 121us/step - loss: 0.0512 - acc: 0.9837\n",
      "Epoch 16/100\n",
      "6425/6425 [==============================] - 1s 120us/step - loss: 0.0476 - acc: 0.9844\n",
      "Epoch 17/100\n",
      "6425/6425 [==============================] - 1s 129us/step - loss: 0.0472 - acc: 0.9852\n",
      "Epoch 18/100\n",
      "6425/6425 [==============================] - 1s 117us/step - loss: 0.0465 - acc: 0.9847\n",
      "Epoch 19/100\n",
      "6425/6425 [==============================] - 1s 108us/step - loss: 0.0399 - acc: 0.9874\n",
      "Epoch 20/100\n",
      "6425/6425 [==============================] - 1s 105us/step - loss: 0.0381 - acc: 0.9883\n",
      "Epoch 21/100\n",
      "6425/6425 [==============================] - 1s 104us/step - loss: 0.0381 - acc: 0.9880\n",
      "Epoch 22/100\n",
      "6425/6425 [==============================] - 1s 105us/step - loss: 0.0330 - acc: 0.9893\n",
      "Epoch 23/100\n",
      "6425/6425 [==============================] - 1s 103us/step - loss: 0.0278 - acc: 0.9916\n",
      "Epoch 24/100\n",
      "6425/6425 [==============================] - 1s 101us/step - loss: 0.0310 - acc: 0.9896\n",
      "Epoch 25/100\n",
      "6425/6425 [==============================] - 1s 104us/step - loss: 0.0276 - acc: 0.9921\n",
      "Epoch 26/100\n",
      "6425/6425 [==============================] - 1s 107us/step - loss: 0.0281 - acc: 0.9913\n",
      "Epoch 27/100\n",
      "6425/6425 [==============================] - 1s 105us/step - loss: 0.0332 - acc: 0.9897\n",
      "Epoch 28/100\n",
      "6425/6425 [==============================] - 1s 119us/step - loss: 0.0231 - acc: 0.9932\n",
      "Epoch 29/100\n",
      "6425/6425 [==============================] - 1s 116us/step - loss: 0.0252 - acc: 0.9919\n",
      "Epoch 30/100\n",
      "6425/6425 [==============================] - 1s 129us/step - loss: 0.0293 - acc: 0.9918\n",
      "Epoch 31/100\n",
      "6425/6425 [==============================] - 1s 131us/step - loss: 0.0212 - acc: 0.9941\n",
      "Epoch 32/100\n",
      "6425/6425 [==============================] - 1s 108us/step - loss: 0.0246 - acc: 0.9935\n",
      "Epoch 33/100\n",
      "6425/6425 [==============================] - 1s 107us/step - loss: 0.0239 - acc: 0.9932\n",
      "Epoch 34/100\n",
      "6425/6425 [==============================] - 1s 111us/step - loss: 0.0230 - acc: 0.9938\n",
      "Epoch 35/100\n",
      "6425/6425 [==============================] - 1s 108us/step - loss: 0.0231 - acc: 0.9921\n",
      "Epoch 36/100\n",
      "6425/6425 [==============================] - 1s 109us/step - loss: 0.0211 - acc: 0.9936\n",
      "Epoch 37/100\n",
      "6425/6425 [==============================] - 1s 111us/step - loss: 0.0224 - acc: 0.9928\n",
      "Epoch 38/100\n",
      "6425/6425 [==============================] - 1s 111us/step - loss: 0.0204 - acc: 0.9936\n",
      "Epoch 39/100\n",
      "6425/6425 [==============================] - 1s 111us/step - loss: 0.0217 - acc: 0.9919\n",
      "Epoch 40/100\n",
      "6425/6425 [==============================] - 1s 110us/step - loss: 0.0265 - acc: 0.9918\n",
      "Epoch 41/100\n",
      "6425/6425 [==============================] - 1s 109us/step - loss: 0.0206 - acc: 0.9944\n",
      "Epoch 42/100\n",
      "6425/6425 [==============================] - 1s 110us/step - loss: 0.0231 - acc: 0.9925\n",
      "Epoch 43/100\n",
      "6425/6425 [==============================] - 1s 109us/step - loss: 0.0197 - acc: 0.9944\n",
      "Epoch 44/100\n",
      "6425/6425 [==============================] - 1s 108us/step - loss: 0.0192 - acc: 0.9949\n",
      "Epoch 45/100\n",
      "6425/6425 [==============================] - 1s 110us/step - loss: 0.0210 - acc: 0.9941\n",
      "Epoch 46/100\n",
      "6425/6425 [==============================] - 1s 109us/step - loss: 0.0206 - acc: 0.9933\n",
      "Epoch 47/100\n",
      "6425/6425 [==============================] - 1s 109us/step - loss: 0.0190 - acc: 0.9946\n",
      "Epoch 48/100\n",
      "6425/6425 [==============================] - 1s 110us/step - loss: 0.0189 - acc: 0.9944\n",
      "Epoch 49/100\n",
      "6425/6425 [==============================] - 1s 114us/step - loss: 0.0189 - acc: 0.9938\n",
      "Epoch 50/100\n",
      "6425/6425 [==============================] - 1s 110us/step - loss: 0.0182 - acc: 0.9947\n",
      "Epoch 51/100\n",
      "6425/6425 [==============================] - 1s 107us/step - loss: 0.0171 - acc: 0.9956\n",
      "Epoch 52/100\n",
      "6425/6425 [==============================] - 1s 110us/step - loss: 0.0211 - acc: 0.9941\n",
      "Epoch 53/100\n",
      "6425/6425 [==============================] - 1s 109us/step - loss: 0.0155 - acc: 0.9960\n",
      "Epoch 54/100\n",
      "6425/6425 [==============================] - 1s 111us/step - loss: 0.0175 - acc: 0.9946\n",
      "Epoch 55/100\n",
      "6425/6425 [==============================] - 1s 109us/step - loss: 0.0216 - acc: 0.9950\n",
      "Epoch 56/100\n",
      "6425/6425 [==============================] - 1s 109us/step - loss: 0.0177 - acc: 0.9947\n",
      "Epoch 57/100\n",
      "6425/6425 [==============================] - 1s 110us/step - loss: 0.0170 - acc: 0.9953\n",
      "Epoch 58/100\n",
      "6425/6425 [==============================] - 1s 109us/step - loss: 0.0160 - acc: 0.9958\n",
      "Epoch 59/100\n",
      "6425/6425 [==============================] - 1s 110us/step - loss: 0.0227 - acc: 0.9946\n",
      "Epoch 60/100\n",
      "6425/6425 [==============================] - 1s 111us/step - loss: 0.0189 - acc: 0.9946\n",
      "Epoch 61/100\n",
      "6425/6425 [==============================] - 1s 110us/step - loss: 0.0195 - acc: 0.9941\n",
      "Epoch 62/100\n",
      "6425/6425 [==============================] - 1s 112us/step - loss: 0.0169 - acc: 0.9942\n",
      "Epoch 63/100\n",
      "6425/6425 [==============================] - 1s 111us/step - loss: 0.0169 - acc: 0.9952\n",
      "Epoch 64/100\n",
      "6425/6425 [==============================] - 1s 109us/step - loss: 0.0147 - acc: 0.9955\n",
      "Epoch 65/100\n",
      "6425/6425 [==============================] - 1s 110us/step - loss: 0.0161 - acc: 0.9949\n",
      "Epoch 66/100\n",
      "6425/6425 [==============================] - 1s 111us/step - loss: 0.0175 - acc: 0.9956\n",
      "Epoch 67/100\n",
      "6425/6425 [==============================] - 1s 110us/step - loss: 0.0149 - acc: 0.9958\n",
      "Epoch 68/100\n",
      "6425/6425 [==============================] - 1s 109us/step - loss: 0.0167 - acc: 0.9950\n",
      "Epoch 69/100\n",
      "6425/6425 [==============================] - 1s 110us/step - loss: 0.0147 - acc: 0.9953\n",
      "Epoch 70/100\n",
      "6425/6425 [==============================] - 1s 112us/step - loss: 0.0161 - acc: 0.9950\n",
      "Epoch 71/100\n",
      "6425/6425 [==============================] - 1s 111us/step - loss: 0.0184 - acc: 0.9950\n",
      "Epoch 72/100\n",
      "6425/6425 [==============================] - 1s 112us/step - loss: 0.0133 - acc: 0.9960\n",
      "Epoch 73/100\n",
      "6425/6425 [==============================] - 1s 109us/step - loss: 0.0170 - acc: 0.9952\n",
      "Epoch 74/100\n",
      "6425/6425 [==============================] - 1s 111us/step - loss: 0.0176 - acc: 0.9939\n",
      "Epoch 75/100\n",
      "6425/6425 [==============================] - 1s 112us/step - loss: 0.0144 - acc: 0.9958\n",
      "Epoch 76/100\n",
      "6425/6425 [==============================] - 1s 111us/step - loss: 0.0158 - acc: 0.9955\n",
      "Epoch 77/100\n",
      "6425/6425 [==============================] - 1s 112us/step - loss: 0.0157 - acc: 0.9947\n",
      "Epoch 78/100\n",
      "6425/6425 [==============================] - 1s 113us/step - loss: 0.0163 - acc: 0.9944\n",
      "Epoch 79/100\n",
      "6425/6425 [==============================] - 1s 112us/step - loss: 0.0161 - acc: 0.9953\n",
      "Epoch 80/100\n",
      "6425/6425 [==============================] - 1s 111us/step - loss: 0.0205 - acc: 0.9942\n",
      "Epoch 81/100\n",
      "6425/6425 [==============================] - 1s 110us/step - loss: 0.0143 - acc: 0.9961\n",
      "Epoch 82/100\n",
      "6425/6425 [==============================] - 1s 94us/step - loss: 0.0187 - acc: 0.9946\n",
      "Epoch 83/100\n",
      "6425/6425 [==============================] - 1s 93us/step - loss: 0.0167 - acc: 0.9947\n",
      "Epoch 84/100\n",
      "6425/6425 [==============================] - 1s 93us/step - loss: 0.0178 - acc: 0.9944\n",
      "Epoch 85/100\n",
      "6425/6425 [==============================] - 1s 95us/step - loss: 0.0169 - acc: 0.9956\n",
      "Epoch 86/100\n",
      "6425/6425 [==============================] - 1s 95us/step - loss: 0.0165 - acc: 0.9958\n",
      "Epoch 87/100\n",
      "6425/6425 [==============================] - 1s 95us/step - loss: 0.0158 - acc: 0.9952\n",
      "Epoch 88/100\n",
      "6425/6425 [==============================] - 1s 96us/step - loss: 0.0138 - acc: 0.9953\n",
      "Epoch 89/100\n",
      "6425/6425 [==============================] - 1s 93us/step - loss: 0.0178 - acc: 0.9946\n",
      "Epoch 90/100\n",
      "6425/6425 [==============================] - 1s 93us/step - loss: 0.0179 - acc: 0.9946\n",
      "Epoch 91/100\n",
      "6425/6425 [==============================] - 1s 93us/step - loss: 0.0173 - acc: 0.9952\n",
      "Epoch 92/100\n",
      "6425/6425 [==============================] - 1s 94us/step - loss: 0.0183 - acc: 0.9944\n",
      "Epoch 93/100\n",
      "6425/6425 [==============================] - 1s 94us/step - loss: 0.0180 - acc: 0.9944\n",
      "Epoch 94/100\n",
      "6425/6425 [==============================] - 1s 92us/step - loss: 0.0174 - acc: 0.9939\n",
      "Epoch 95/100\n",
      "6425/6425 [==============================] - 1s 98us/step - loss: 0.0152 - acc: 0.9955\n",
      "Epoch 96/100\n",
      "6425/6425 [==============================] - 1s 116us/step - loss: 0.0183 - acc: 0.9953\n",
      "Epoch 97/100\n",
      "6425/6425 [==============================] - 1s 121us/step - loss: 0.0168 - acc: 0.9956\n",
      "Epoch 98/100\n",
      "6425/6425 [==============================] - 1s 121us/step - loss: 0.0199 - acc: 0.9944\n",
      "Epoch 99/100\n",
      "6425/6425 [==============================] - 1s 127us/step - loss: 0.0151 - acc: 0.9953\n",
      "Epoch 100/100\n",
      "6425/6425 [==============================] - 1s 121us/step - loss: 0.0187 - acc: 0.9949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd5191ae0b8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile & Train the network on whole learning set\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=50, restore_best_weights=True)\n",
    "callbacks = [lrate]\n",
    "\n",
    "model.compile(loss=loss_func, \n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics)\n",
    "\n",
    "model.fit(learn_data, learn_labels,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_predictions = model.predict(test_data, batch_size=None, verbose=0, steps=None)\n",
    "hard_predictions = np.argmax(prob_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"your_submission.txt\", hard_predictions, fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "03714713"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

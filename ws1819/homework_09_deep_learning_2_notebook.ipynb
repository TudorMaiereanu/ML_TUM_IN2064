{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming assignment 09: Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the skeleton for learning a feed-forward neural network is given. Your task is to complete the functions where required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit your notebook by January 6, 2019** as a .html file with your code and the respective cell outputs.\n",
    "\n",
    "You can also take part in our challenge to fit the best model to the cell phone data by submitting your predictions for the test data. The winners of the challenge will be announced after the Christmas break! See the bottom of this file for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I this assignment we will work with a dataset containing accelerometer and gyroscope sensor signals of a cell-phone. Given the sensor signals, the task is to predict the activity that the cell-phone user is doing, e.g, running, walking, biking, or moving upstairs.\n",
    "\n",
    "The data consists of 8032 samples. Each feature vector contains 6 measurements from the cell-phone sensors in 20 consequent time steps. Indeed, each feature vector is 120-dimensional. The task is to classify these instances into the following classes: \"Walking\", \"Standing\", \"Sitting\", \"Running\", \"Upstairs\", \"Downstairs\".\n",
    "\n",
    "More details can be found here: https://becominghuman.ai/deep-learning-for-sensor-based-human-activity-recognition-970ff47c6b6b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before you start** download the data from https://syncandshare.lrz.de/dl/fiXnRq4MRd8fGJBXH2jQmNcN/homework_09_data.npz and place the file into the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = np.load(\"homework_09_data.npz\")\n",
    "train_data = loader['train_data']\n",
    "train_labels = loader['train_labels']\n",
    "\n",
    "val_data = loader['val_data']\n",
    "val_labels = loader['val_labels']\n",
    "\n",
    "test_data = loader['test_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skeleton of the class ```FeedForwardNet``` is provided in the following. This class implements a feed-forward neural network in Tensorflow. \n",
    "**Your task** is to complete the parts where it says ```### YOUR CODE HERE ###```. \n",
    "\n",
    "To complete the code properly, make sure that you make the computation graph based on the placeholders ```self.X``` and ```self.Y```. These two placeholders are created in the `build` function; you don't need to create them. You only need to use them.\n",
    "* ```self.X```: a placeholder of shape ```[None,D]``` where the none dimension will be replaced by the number of instances, and $D$ is number of features.\n",
    "* ```self.Y```: a placeholder of shape ```[None,K]``` where the none dimension will be replaced by the number of instances, and $K$ is number of classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(num_data, batch_size):\n",
    "    \"\"\" Yield batches with indices until epoch is over.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_data: int\n",
    "        The number of samples in the dataset.\n",
    "    batch_size: int\n",
    "        The batch size used using training.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    batch_ixs: np.array of ints with shape [batch_size,]\n",
    "        Yields arrays of indices of size of the batch size until the epoch is over.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_ixs = np.random.permutation(np.arange(num_data))\n",
    "    ix = 0\n",
    "    while ix + batch_size < num_data:\n",
    "        batch_ixs = data_ixs[ix:ix+batch_size]\n",
    "        ix += batch_size\n",
    "        yield batch_ixs\n",
    "    \n",
    "\n",
    "class FeedForwardNet:\n",
    "    \"\"\"\n",
    "    Simple feed forward neural network class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_sizes, layer_types, name, l2_reg=0.0):\n",
    "        \"\"\" FeedForwardNet constructor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_sizes: list of ints\n",
    "            The sizes of the hidden layers of the network.\n",
    "        name: str\n",
    "            The name of the network (used for a VariableScope)\n",
    "        l2_reg: float\n",
    "            The strength of L2 regularization (0 means no regularization)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.layer_types = layer_types\n",
    "        self.name = name\n",
    "        self.dropout = tf.placeholder_with_default(0.0, shape=(), name=\"dropout\")\n",
    "        self.l2_reg = l2_reg\n",
    "        self.weights =[]\n",
    "        self.biases =[]\n",
    "    \n",
    "    def build(self, data_dim, num_classes):\n",
    "        \"\"\" Construct the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data_dim: int\n",
    "            The dimensions of the data samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        self.X = tf.placeholder(shape=[None, data_dim], dtype=tf.float32, name=\"data\") #[NxD]\n",
    "        self.Y = tf.placeholder(shape=[None, num_classes], dtype=tf.float32, name=\"labels\") #[Nx1]\n",
    "    \n",
    "        with tf.variable_scope(self.name):\n",
    "        \n",
    "            hidden = self.X\n",
    "\n",
    "            for ix, hidden_size in enumerate(self.hidden_sizes):\n",
    "                ### YOUR CODE HERE ###\n",
    "                hidden = ### YOUR CODE HERE ###\n",
    "                    \n",
    "            ### YOUR CODE HERE ###\n",
    "            \n",
    "            self.logits = ### YOUR CODE HERE ###\n",
    "            self.l2_norm = ### YOUR CODE HERE ###\n",
    "            self.cross_entropy_loss = ### YOUR CODE HERE ###\n",
    "            self.accuracy = ### YOUR CODE HERE ###\n",
    "            \n",
    "            self.loss = ### YOUR CODE HERE ###\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer()\n",
    "            self.opt_op = self.optimizer.minimize(self.loss, var_list=[*self.weights, *self.biases])\n",
    "            \n",
    "        \n",
    "    def train(self, train_data, train_labels, val_data, val_labels, epochs=20, dropout=0.0, batch_size=512):\n",
    "        \"\"\" Train the feed forward neural network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data: np.array, dtype float32, shape [N, D]\n",
    "            The training data. N corresponds to the number of training samples, D to the dimensionality of the data samples/\n",
    "        train_labels: np.array, shape [N, K]\n",
    "            The labels of the training data, where K is the number of classes.\n",
    "        val_data: np.array, dtype float32, shape [N_val, D]\n",
    "            The validation data. N_val corresponds to the number of validation samples, D to the dimensionality of the data samples/\n",
    "        val_labels: np.array, shape [N_val, K]\n",
    "            The labels of the training data, where K is the number of classes.\n",
    "        epochs: int\n",
    "            The number of epochs to train for.\n",
    "        dropout: float\n",
    "            The dropout rate used during training. 0 corresponds to no dropout.\n",
    "        batch_size: int\n",
    "            The batch size used for training.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        val_losses = []\n",
    "        val_accs = []\n",
    "        \n",
    "        self.session = tf.Session()\n",
    "        session = self.session\n",
    "        \n",
    "        with session.as_default():\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            \n",
    "            tr_loss, tr_acc= session.run(### YOUR CODE HERE ###)\n",
    "            val_loss, val_acc= session.run(### YOUR CODE HERE ###)\n",
    "                \n",
    "            train_losses.append(tr_loss)\n",
    "            train_accs.append(tr_acc)\n",
    "            \n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "                        \n",
    "            for epoch in range(epochs):\n",
    "                if (epoch + 1) % 25 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "                for batch_ixs in batch_data(len(train_data), batch_size):\n",
    "                    _ = session.run(### YOUR CODE HERE ###)\n",
    "                    \n",
    "                tr_loss, tr_acc= session.run(### YOUR CODE HERE ###)\n",
    "                val_loss, val_acc= session.run(### YOUR CODE HERE ###)\n",
    "                train_losses.append(tr_loss)\n",
    "                train_accs.append(tr_acc)\n",
    "\n",
    "                val_losses.append(val_loss)\n",
    "                val_accs.append(val_acc)    \n",
    "\n",
    "    \n",
    "        self.hist={'train_loss': np.array(train_losses),\n",
    "           'train_accuracy': np.array(train_accs),\n",
    "           'val_loss': np.array(val_losses),\n",
    "           'val_accuracy': np.array(val_accs)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, specify the FFNN. To do so, you can set the following fields in the next cell:\n",
    "* ```hidden_sizes```: a list that contains the number of hidden neurons in different layers.\n",
    "* ```layer_types```: a list containing the activation functions of the layers. \n",
    "\n",
    "For instance, the values of the following cell specify a FFNN having 3 ReLU layers and a softmax layer. Note that we do not explicitly mention 'softmax'; Because we know that for the classification task, the last layer is softmax. Moreover, we do not specify $D$ and $K$ in the variable ```hidden_sizes```, because we know that the first layer has $D$ neurons and the last layer has $K$ neurons. \n",
    "\n",
    "\n",
    "You can change the configuration of the network. The sample solution is built by the following configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start without any regularization. You can set the values ```num_epochs``` and ```batch_size``` in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can change layer types and the number of neurons by changing the following variables.\n",
    "layer_types = [tf.nn.relu, tf.nn.relu, tf.nn.relu]\n",
    "hidden_sizes = [64, 32, 16]\n",
    "epochs = 250\n",
    "batch_size = 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network Using Different Regularizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we learn the neural network in three different settings:\n",
    "* Without any regularization\n",
    "* With $\\ell 2$ regularization\n",
    "* With dropout\n",
    "\n",
    "For each case, we are going to see how the training/validation/test losses will change during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_no_regularization = FeedForwardNet(hidden_sizes, layer_types, \"no_regularization\")\n",
    "NN_no_regularization.build(train_data.shape[1], num_classes=train_labels.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/250\n",
      "Epoch 50/250\n",
      "Epoch 75/250\n",
      "Epoch 100/250\n",
      "Epoch 125/250\n",
      "Epoch 150/250\n",
      "Epoch 175/250\n",
      "Epoch 200/250\n",
      "Epoch 225/250\n",
      "Epoch 250/250\n"
     ]
    }
   ],
   "source": [
    "NN_no_regularization.train(train_data, train_labels, val_data, val_labels, epochs,\n",
    "                          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the training and validation losses over the epochs. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(NN_no_regularization.hist['train_loss'][5::], label=\"Training\")\n",
    "plt.plot(NN_no_regularization.hist['val_loss'][5::], label=\"Validation\")\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=20)\n",
    "plt.ylabel(\"Loss\", fontsize=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now plot the training and validation accuracies over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(NN_no_regularization.hist['train_accuracy'])\n",
    "plt.plot(NN_no_regularization.hist['val_accuracy'])\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\ell 2$ regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we use $\\ell 2$ regularization, and we investigate the train/validation/test loss. Set the regularization parameter to 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_L2_regularization = FeedForwardNet(hidden_sizes, layer_types, \"L2_regularization\", l2_reg=1e-2)\n",
    "NN_L2_regularization.build(train_data.shape[1], num_classes=train_labels.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/250\n",
      "Epoch 50/250\n",
      "Epoch 75/250\n",
      "Epoch 100/250\n",
      "Epoch 125/250\n",
      "Epoch 150/250\n",
      "Epoch 175/250\n",
      "Epoch 200/250\n",
      "Epoch 225/250\n",
      "Epoch 250/250\n"
     ]
    }
   ],
   "source": [
    "NN_L2_regularization.train(train_data, train_labels, val_data, val_labels, epochs,\n",
    "                          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train a model using dropout. Use a dropout rate of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_dropout_regularization = FeedForwardNet(hidden_sizes, layer_types, \"dropout_regularization\")\n",
    "NN_dropout_regularization.build(train_data.shape[1], num_classes=train_labels.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/250\n",
      "Epoch 50/250\n",
      "Epoch 75/250\n",
      "Epoch 100/250\n",
      "Epoch 125/250\n",
      "Epoch 150/250\n",
      "Epoch 175/250\n",
      "Epoch 200/250\n",
      "Epoch 225/250\n",
      "Epoch 250/250\n"
     ]
    }
   ],
   "source": [
    "NN_dropout_regularization.train(train_data, train_labels, val_data, val_labels, epochs,\n",
    "                          batch_size=batch_size, dropout=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the models\n",
    "Now, compare the final training and validation losses achieved by the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_noreg = NN_no_regularization.hist['train_accuracy'][-1]\n",
    "val_acc_noreg = NN_no_regularization.hist['val_accuracy'][-1]\n",
    "\n",
    "train_acc_L2reg = NN_L2_regularization.hist['train_accuracy'][-1]\n",
    "val_acc_L2reg = NN_L2_regularization.hist['val_accuracy'][-1]\n",
    "\n",
    "train_acc_dropoutreg = NN_dropout_regularization.hist['train_accuracy'][-1]\n",
    "val_acc_dropoutreg = NN_dropout_regularization.hist['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy without regularization: XXX\n",
      "Validation accuracy without regularization: XXX\n",
      "\n",
      "Training accuracy with L2 regularization: XXX\n",
      "Validation accuracy with L2 regularization: XXX\n",
      "\n",
      "Training accuracy with dropout regularization: XXX\n",
      "Validation accuracy with dropout regularization: XXX\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training accuracy without regularization: {train_acc_noreg:.3f}\")\n",
    "print(f\"Validation accuracy without regularization: {val_acc_noreg:.3f}\")\n",
    "print()\n",
    "\n",
    "print(f\"Training accuracy with L2 regularization: {train_acc_L2reg:.3f}\")\n",
    "print(f\"Validation accuracy with L2 regularization: {val_acc_L2reg:.3f}\")\n",
    "print()\n",
    "\n",
    "print(f\"Training accuracy with dropout regularization: {train_acc_dropoutreg:.3f}\")\n",
    "print(f\"Validation accuracy with dropout regularization: {val_acc_dropoutreg:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the losses and accuracies of the models in one plot to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(NN_no_regularization.hist['train_loss'][5::], label=\"Training (no regularization)\",\n",
    "        color=\"darkgreen\")\n",
    "plt.plot(NN_no_regularization.hist['val_loss'][5::], label=\"Validation (no regularization)\",\n",
    "        color=\"darkgreen\", linestyle=\"--\")\n",
    "\n",
    "plt.plot(NN_L2_regularization.hist['train_loss'][5::], label=\"Training (L2 regularization)\",\n",
    "        color=\"royalblue\")\n",
    "plt.plot(NN_L2_regularization.hist['val_loss'][5::], label=\"Validation (L2 regularization)\",\n",
    "        color=\"royalblue\", linestyle=\"--\")\n",
    "\n",
    "plt.plot(NN_dropout_regularization.hist['train_loss'][5::], \n",
    "         label=\"Training (dropout regularization)\", color=\"purple\")\n",
    "plt.plot(NN_dropout_regularization.hist['val_loss'][5::], \n",
    "         label=\"Validation (dropout regularization)\", color=\"purple\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=20)\n",
    "plt.ylabel(\"Loss\", fontsize=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(NN_no_regularization.hist['train_accuracy'], label=\"Training (no regularization)\",\n",
    "        color=\"darkgreen\")\n",
    "plt.plot(NN_no_regularization.hist['val_accuracy'], label=\"Validation (no regularization)\",\n",
    "        color=\"darkgreen\", linestyle=\"--\")\n",
    "\n",
    "plt.plot(NN_L2_regularization.hist['train_accuracy'], label=\"Training (L2 regularization)\",\n",
    "        color=\"royalblue\")\n",
    "plt.plot(NN_L2_regularization.hist['val_accuracy'], label=\"Validation (L2 regularization)\",\n",
    "        color=\"royalblue\", linestyle=\"--\")\n",
    "\n",
    "plt.plot(NN_dropout_regularization.hist['train_accuracy'], \n",
    "         label=\"Training (dropout regularization)\", color=\"purple\")\n",
    "plt.plot(NN_dropout_regularization.hist['val_accuracy'], \n",
    "         label=\"Validation (dropout regularization)\", color=\"purple\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", fontsize=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we also have the variable `test_data`. **Get creative** and build a model yourself!\n",
    "\n",
    "Submit your predictions of the test data to Moodle to enter the leaderboard. We will announce the winning team after the Christmas break!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can output the predictions of the test data using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = YOUR_NN_MODEL.logits.eval({YOUR_NN_MODEL.X: test_data},\n",
    "                                        session=YOUR_NN_MODEL.session).argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only consider submission as plain text files with **exactly** the following formatting as our `sample_submission.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "3\n",
      "0\n",
      "0\n",
      "4\n",
      "5\n",
      "1\n",
      "2\n",
      "4\n",
      "3\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "string = \"\"\n",
    "with open(\"sample_submission.txt\", \"r\") as f:\n",
    "    string = f.read()\n",
    "print(string[:19])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following command to save your predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"your_submission.txt\", test_preds, fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

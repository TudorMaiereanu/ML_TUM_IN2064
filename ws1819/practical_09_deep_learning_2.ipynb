{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN2064 Practical Session 9 -- Deep Learning 2\n",
    "\n",
    "In this programming exercise your task is to use Tensorflow to classify reviews in the IMDB dataset. The possible labels are positive and negative, i.e. the task is binary classification.\n",
    "\n",
    "1. Fill in the required parts of the skeleton below.\n",
    "1. Train feed forward neural networks of different sizes (specified below) on the IMDB dataset\n",
    "1. Use Tensorboard to visualize the computation graph.\n",
    "1. Plot the training and validation losses for the different neural networks. What are the influences of the neural network capacity, L2 regularization and dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the data from https://syncandshare.lrz.de/dl/fiWqeMXAwSqt4Nr3u6b1Jr9S/imdb_data.npz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = np.load(\"imdb_data.npz\")\n",
    "train_data = loader['train_data']\n",
    "train_labels = loader['train_labels']\n",
    "validation_data = loader['validation_data']\n",
    "validation_labels = loader['validation_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(num_data, batch_size):\n",
    "    \"\"\" Yield batches with indices until epoch is over.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_data: int\n",
    "        The number of samples in the dataset.\n",
    "    batch_size: int\n",
    "        The batch size used using training.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    batch_ixs: np.array of ints with shape [batch_size,]\n",
    "        Yields arrays of indices of size of the batch size until the epoch is over.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_ixs = np.random.permutation(np.arange(num_data))\n",
    "    ix = 0\n",
    "    while ix + batch_size < num_data:\n",
    "        batch_ixs = data_ixs[ix:ix+batch_size]\n",
    "        ix += batch_size\n",
    "        yield batch_ixs\n",
    "    \n",
    "\n",
    "class FeedForwardNet:\n",
    "    \"\"\"\n",
    "    Simple feed forward neural network class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_sizes, name, l2_reg=0.0):\n",
    "        \"\"\" FeedForwardNet constructor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_sizes: list of ints\n",
    "            The sizes of the hidden layers of the network.\n",
    "        name: str\n",
    "            The name of the network (used for a VariableScope)\n",
    "        l2_reg: float\n",
    "            The strength of L2 regularization (0 means no regularization)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.name = name\n",
    "        self.dropout = tf.placeholder_with_default(0.0, shape=(), name=\"dropout\")\n",
    "        self.l2_reg = l2_reg\n",
    "        self.weights =[]\n",
    "        self.biases =[]\n",
    "    \n",
    "    def build(self, data_dim):\n",
    "        \"\"\" Construct the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data_dim: int\n",
    "            The dimensions of the data samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        self.X = tf.placeholder(shape=[None, data_dim], dtype=tf.float32, name=\"data\") #[NxD]\n",
    "        self.Y = tf.placeholder(shape=[None, 1], dtype=tf.float32, name=\"labels\") #[Nx1]\n",
    "    \n",
    "        with tf.variable_scope(self.name):\n",
    "        \n",
    "            hidden = self.X\n",
    "\n",
    "            for ix, hidden_size in enumerate(self.hidden_sizes):\n",
    "                ### YOUR CODE HERE \n",
    "\n",
    "            ### YOUR CODE HERE ###     \n",
    "            \n",
    "            self.logits = ### YOUR CODE HERE ###\n",
    "            self.l2_norm =  ### YOUR CODE HERE ###\n",
    "            self.cross_entropy_loss =  ### YOUR CODE HERE ###\n",
    "            self.accuracy = ### YOUR CODE HERE ###\n",
    "            \n",
    "            self.loss = ### YOUR CODE HERE ###\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer()\n",
    "            self.opt_op = self.optimizer.minimize(self.loss, var_list=[*self.weights, *self.biases])\n",
    "            \n",
    "    def number_trainable_parameters(self):\n",
    "        \"\"\" Compute number of trainable parameters in the model.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        num_params: int\n",
    "            The number of trainable parameters.\n",
    "        \"\"\"\n",
    "        num_params = np.sum([np.prod(x.shape) for x in self.weights])\n",
    "        num_params += np.sum([np.prod(x.shape) for x in self.biases])\n",
    "        return int(num_params)\n",
    "        \n",
    "    def train(self, train_data, train_labels, val_data, val_labels, epochs=20, dropout=0.0, batch_size=512):\n",
    "        \"\"\" Train the feed forward neural network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data: np.array, dtype float32, shape [N, D]\n",
    "            The training data. N corresponds to the number of training samples, D to the dimensionality of the data samples/\n",
    "        train_labels: np.array, shape [N, 1]\n",
    "            The labels of the training data.\n",
    "        val_data: np.array, dtype float32, shape [N_val, D]\n",
    "            The validation data. N_val corresponds to the number of validation samples, D to the dimensionality of the data samples/\n",
    "        val_labels: np.array, shape [N_val, 1]\n",
    "            The labels of the training data.\n",
    "        epochs: int\n",
    "            The number of epochs to train for.\n",
    "        dropout: float\n",
    "            The dropout rate used during training. 0 corresponds to no dropout.\n",
    "        batch_size: int\n",
    "            The batch size used for training.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        val_losses = []\n",
    "        val_accs = []\n",
    "        weight_norms = []\n",
    "        \n",
    "        self.session = tf.Session()\n",
    "        session = self.session\n",
    "        \n",
    "        with session.as_default():\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            \n",
    "            \n",
    "            \n",
    "            tr_loss, tr_acc=     ### YOUR CODE HERE ###\n",
    "            val_loss, val_acc=   ### YOUR CODE HERE ###\n",
    "            train_losses.append(tr_loss)\n",
    "            train_accs.append(tr_acc)\n",
    "            \n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            weight_norms.append(session.run(self.l2_norm))\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "                for batch_ixs in batch_data(len(train_data), batch_size):\n",
    "                    _ = session.run( ### YOUR CODE HERE ### )\n",
    "                    \n",
    "                tr_loss, tr_acc= session.run(### YOUR CODE HERE ###)\n",
    "                val_loss, val_acc= session.run(### YOUR CODE HERE ###)\n",
    "                train_losses.append(tr_loss)\n",
    "                train_accs.append(tr_acc)\n",
    "\n",
    "                val_losses.append(val_loss)\n",
    "                val_accs.append(val_acc)    \n",
    "                weight_norms.append(session.run(self.l2_norm))\n",
    "\n",
    "    \n",
    "        self.hist={'train_loss': np.array(train_losses),\n",
    "           'train_accuracy': np.array(train_accs),\n",
    "           'val_loss': np.array(val_losses),\n",
    "           'val_accuracy': np.array(val_accs),\n",
    "           'weight_norms': np.array(weight_norms)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small neural network has 4029 trainable parameters.\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n"
     ]
    }
   ],
   "source": [
    "NN_small = FeedForwardNet([4,4], \"small\")\n",
    "NN_small.build(train_data.shape[1])\n",
    "print(f\"{NN_small.name} neural network has {NN_small.number_trainable_parameters()} trainable parameters.\")\n",
    "NN_small.train(train_data, train_labels, validation_data, validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.FileWriter('logs',graph=NN_small.session.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medium neural network has 16305 trainable parameters.\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n"
     ]
    }
   ],
   "source": [
    "NN_medium = FeedForwardNet([16,16], \"medium\")\n",
    "NN_medium.build(train_data.shape[1])\n",
    "print(f\"{NN_medium.name} neural network has {NN_medium.number_trainable_parameters()} trainable parameters.\")\n",
    "NN_medium.train(train_data, train_labels, validation_data, validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large neural network has 775681 trainable parameters.\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n"
     ]
    }
   ],
   "source": [
    "NN_large = FeedForwardNet([512,512], \"large\")\n",
    "NN_large.build(train_data.shape[1])\n",
    "print(f\"{NN_large.name} neural network has {NN_large.number_trainable_parameters()} trainable parameters.\")\n",
    "NN_large.train(train_data, train_labels, validation_data, validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large_reg neural network has 775681 trainable parameters.\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n"
     ]
    }
   ],
   "source": [
    "NN_large_reg = FeedForwardNet([512,512], \"large_reg\", l2_reg=1e-2)\n",
    "NN_large_reg.build(train_data.shape[1])\n",
    "print(f\"{NN_large_reg.name} neural network has {NN_large_reg.number_trainable_parameters()} trainable parameters.\")\n",
    "NN_large_reg.train(train_data, train_labels, validation_data, validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large_dropout2 neural network has 775681 trainable parameters.\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n"
     ]
    }
   ],
   "source": [
    "NN_large_dropout = FeedForwardNet([512,512], \"large_dropout\")\n",
    "NN_large_dropout.build(train_data.shape[1])\n",
    "print(f\"{NN_large_dropout.name} neural network has {NN_large_dropout.number_trainable_parameters()} trainable parameters.\")\n",
    "NN_large_dropout.train(train_data, train_labels, validation_data, validation_labels, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.plot(NN_small.hist['train_loss'], label=\"Training (small)\", c=\"darkgreen\")\n",
    "plt.plot(NN_small.hist['val_loss'], label=\"Validation (small)\", c=\"darkgreen\", linestyle=\"--\")\n",
    "\n",
    "plt.plot(NN_medium.hist['train_loss'], label=\"Training (medium)\", c=\"royalblue\")\n",
    "plt.plot(NN_medium.hist['val_loss'], label=\"Validation (medium)\", c=\"royalblue\", linestyle=\"--\")\n",
    "\n",
    "plt.plot(NN_large.hist['train_loss'], label=\"Training (large)\", c=\"darkred\")\n",
    "plt.plot(NN_large.hist['val_loss'], label=\"Validation (large)\", c=\"darkred\", linestyle=\"--\")\n",
    "\n",
    "plt.plot(NN_large_reg.hist['train_loss'], label=\"Training (large w/ regularization)\", c=\"orange\")\n",
    "plt.plot(NN_large_reg.hist['val_loss'], label=\"Validation (large w/ regularization)\", c=\"orange\", linestyle=\"--\")\n",
    "\n",
    "plt.plot(NN_large_dropout.hist['train_loss'], label=\"Training (large w/ dropout)\", c=\"purple\")\n",
    "plt.plot(NN_large_dropout.hist['val_loss'], label=\"Validation (large w/ dropout)\", c=\"purple\", linestyle=\"--\")\n",
    "\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.plot(NN_small.hist['train_accuracy'], label=\"Training (small)\", c=\"darkgreen\")\n",
    "plt.plot(NN_small.hist['val_accuracy'], label=\"Validation (small)\", c=\"darkgreen\", linestyle=\"--\")\n",
    "\n",
    "plt.plot(NN_medium.hist['train_accuracy'], label=\"Training (medium)\", c=\"royalblue\")\n",
    "plt.plot(NN_medium.hist['val_accuracy'], label=\"Validation (medium)\", c=\"royalblue\", linestyle=\"--\")\n",
    "\n",
    "plt.plot(NN_large.hist['train_accuracy'], label=\"Training (large)\", c=\"darkred\")\n",
    "plt.plot(NN_large.hist['val_accuracy'], label=\"Validation (large)\", c=\"darkred\", linestyle=\"--\")\n",
    "\n",
    "plt.plot(NN_large_reg.hist['train_accuracy'], label=\"Training (large w/ regularization)\", c=\"orange\")\n",
    "plt.plot(NN_large_reg.hist['val_accuracy'], label=\"Validation (large w/ regularization)\", c=\"orange\", linestyle=\"--\")\n",
    "\n",
    "plt.plot(NN_large_dropout.hist['train_accuracy'], label=\"Training (large w/ dropout)\", c=\"purple\")\n",
    "plt.plot(NN_large_dropout.hist['val_accuracy'], label=\"Validation (large w/ dropout)\", c=\"purple\", linestyle=\"--\")\n",
    "\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.plot(NN_large.hist['weight_norms'], label=\"Weight norm (large)\", c=\"darkred\")\n",
    "plt.plot(NN_large_reg.hist['weight_norms'], label=\"Weight norm (large w/ regularization)\", c=\"orange\")\n",
    "\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('L2 norm of weights')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
